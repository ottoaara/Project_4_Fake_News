{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61345247",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "#### Aaron Otto, Ayan Khalif, Kirsten Rain, Ryan Cornelius\n",
    "\n",
    "\n",
    "##### See readme\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb34f59",
   "metadata": {},
   "source": [
    "## Part 1 \n",
    "\n",
    "#### Loads in a tab seperated dataset of new data. Data was found labelled from https://builtin.com/data-science/data-science-projects (link in number 3)\n",
    "##### This file was initially a CSV in ANSI format, and was converted to a TSV in UTF-8 using notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa93d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "# Libraries for data cleaning\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import re \n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Libraries for NN W2V model\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "import nltk\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#\n",
    "#Installing profiling lib (figured no one else would have this so doing pip incase)\n",
    "#!pip install -U ydata-profiling\n",
    "#!pip install ipywidgets\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "# Turning Memory warnings off\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287c054d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Scalia’s death comes just a month before the c...</td>\n",
       "      <td>The unexpected death of Justice Antonin Scalia...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Rachel Dolezal's brother: She's 'making up mor...</td>\n",
       "      <td>(CNN) Ezra Dolezal would love to see his siste...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>What A Hillary Presidency Would Bring</td>\n",
       "      <td>Behind the headlines - conspiracies, cover-ups...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>Hillary Endorsed Donald Trump for President Ac...</td>\n",
       "      <td>Hillary Endorsed Donald Trump for President Ac...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10006</td>\n",
       "      <td>US-led coalition killed 300 Syrian civilians i...</td>\n",
       "      <td>RT October 26, 2016 \\r\\nAround 300 civilians w...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     key  ... label\n",
       "0      1  ...  REAL\n",
       "1      1  ...  REAL\n",
       "2  10001  ...  FAKE\n",
       "3  10004  ...  FAKE\n",
       "4  10006  ...  FAKE\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Data File\n",
    "df = pd.read_table(\"news-utf8.tsv\", delimiter = '\\t')\n",
    "\n",
    "\n",
    "#Removing unecessary columns \n",
    "df =df[['key','title','text','label']]\n",
    "\n",
    "#Dropping any blank rows picked up in excel load\n",
    "df=df.dropna(how ='all')\n",
    "df= df.dropna(subset=['text'])\n",
    "\n",
    "#Formating Key column \n",
    "df['key'] = df['key'].astype(str).str.rstrip('0').str.rstrip('.')\n",
    "\n",
    "#Sorting key column \n",
    "df=df.sort_values(by='key')\n",
    "\n",
    "#Resetting index to correspond with key values asscending \n",
    "df= df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f294d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop_list = list(ENGLISH_STOP_WORDS.copy())\n",
    "Stop_list.extend(['s', 't', 'i', 'm', 'd', 'u', 've'])\n",
    "Stop_list.extend(list(np.arange(0,10)))\n",
    "Stop_list = set(Stop_list)\n",
    "ENGLISH_STOP_WORDS = Stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d625ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Words_Text</th>\n",
       "      <th>Words_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Scalia’s death comes just a month before the c...</td>\n",
       "      <td>The unexpected death of Justice Antonin Scalia...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[the, unexpected, death, of, justice, antonin,...</td>\n",
       "      <td>[scalia, s, death, comes, just, a, month, befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Rachel Dolezal's brother: She's 'making up mor...</td>\n",
       "      <td>(CNN) Ezra Dolezal would love to see his siste...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[cnn, ezra, dolezal, would, love, to, see, his...</td>\n",
       "      <td>[rachel, dolezal, s, brother, she, s, making, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>What A Hillary Presidency Would Bring</td>\n",
       "      <td>Behind the headlines - conspiracies, cover-ups...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[behind, the, headlines, conspiracies, cover, ...</td>\n",
       "      <td>[what, a, hillary, presidency, would, bring]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>Hillary Endorsed Donald Trump for President Ac...</td>\n",
       "      <td>Hillary Endorsed Donald Trump for President Ac...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[hillary, endorsed, donald, trump, for, presid...</td>\n",
       "      <td>[hillary, endorsed, donald, trump, for, presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10006</td>\n",
       "      <td>US-led coalition killed 300 Syrian civilians i...</td>\n",
       "      <td>RT October 26, 2016 \\r\\nAround 300 civilians w...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[rt, october, 26, 2016, around, 300, civilians...</td>\n",
       "      <td>[us, led, coalition, killed, 300, syrian, civi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     key  ...                                        Words_Title\n",
       "0      1  ...  [scalia, s, death, comes, just, a, month, befo...\n",
       "1      1  ...  [rachel, dolezal, s, brother, she, s, making, ...\n",
       "2  10001  ...       [what, a, hillary, presidency, would, bring]\n",
       "3  10004  ...  [hillary, endorsed, donald, trump, for, presid...\n",
       "4  10006  ...  [us, led, coalition, killed, 300, syrian, civi...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize 'text' and 'title' columns and store the result\n",
    "df['Words_Text'] = df['text'].apply(lambda x: re.findall(r'\\b\\w+\\b', str(x).lower()))\n",
    "df['Words_Title'] = df['title'].apply(lambda x: re.findall(r'\\b\\w+\\b', str(x).lower()))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove stop words from a list of words and join them with commas\n",
    "def process_words(words, stop_words):\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    return list(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5062dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and join words with commas for 'Words_Text' and 'Words_Title' columns\n",
    "df['Words_Text'] = df['Words_Text'].apply(lambda x: process_words(x, ENGLISH_STOP_WORDS))\n",
    "df['Words_Title'] = df['Words_Title'].apply(lambda x: process_words(x, ENGLISH_STOP_WORDS))\n",
    "df.drop(labels = ['title', 'text', 'key'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'Words_Text': \"text\", \n",
    "                   'Words_Title': 'title'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bcb9be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>[unexpected, death, justice, antonin, scalia, ...</td>\n",
       "      <td>[scalia, death, comes, just, month, court, big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>[cnn, ezra, dolezal, love, sister, rachel, dna...</td>\n",
       "      <td>[rachel, dolezal, brother, making, lies]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>[headlines, conspiracies, cover, ups, ancient,...</td>\n",
       "      <td>[hillary, presidency, bring]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>[hillary, endorsed, donald, trump, president, ...</td>\n",
       "      <td>[hillary, endorsed, donald, trump, president, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>[october, 2016, 300, civilians, killed, airstr...</td>\n",
       "      <td>[led, coalition, killed, 300, syrian, civilian...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  ...                                              title\n",
       "0   True  ...  [scalia, death, comes, just, month, court, big...\n",
       "1   True  ...           [rachel, dolezal, brother, making, lies]\n",
       "2  False  ...                       [hillary, presidency, bring]\n",
       "3  False  ...  [hillary, endorsed, donald, trump, president, ...\n",
       "4  False  ...  [led, coalition, killed, 300, syrian, civilian...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_table(\"current/news_cleaned_utf8.tsv\", delimiter = '\\t')\n",
    "#df.drop(columns=df.columns[0], axis=1,  inplace=True)\n",
    "df['label'].replace({'REAL': True,\n",
    "                     'FAKE' : False}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274945d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.bool_"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cb228",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation\n",
    "\n",
    "#### All setups here, file will be exported labelled with accuracy and number of epochs required\n",
    "##### W2V Documentation used: \n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "##### Tutorials used: \n",
    "https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e\n",
    "\n",
    "http://ethen8181.github.io/machine-learning/keras/text_classification/word2vec_text_classification.html#Gensim-Implementation\n",
    "\n",
    "https://www.kaggle.com/code/atishadhikari/fake-news-cleaning-word2vec-lstm-99-accuracy\n",
    "\n",
    "https://intellipaat.com/blog/what-is-lstm/\n",
    "\n",
    "https://intothedepthsofdataengineering.wordpress.com/2017/06/26/an-overview-of-word2vec/\n",
    "\n",
    "##### As well as in-class materials. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef68a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V Model settings\n",
    "source = 'text' #title or text, whether to run W2V on the article's title or bulk text\n",
    "cores = 6 #Number of CPU cores\n",
    "init_vectorsize = 150 #set the dimensions of the vectors used to represent the word set\n",
    "window = 5 #Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 2 #Ignores all words with total frequency lower than this.\n",
    "max_vocab_size = 50000000 #Every 10 million word types need about 1GB of RAM\n",
    "\n",
    "train_epochs = 25 #number of times to iterate over dataset during training\n",
    "w2v_epochs = train_epochs #number of times to iterate over dataset during W2V. Try to keep same size as train epochs\n",
    "test_size = 0.2 #ratio to split test dataset during train, test split\n",
    "max_vectorsize = 1000 #vectorsize padding to equalize final vectors\n",
    "\n",
    "# NN settings, will be a sequential model with an embedding layer, optional LSTM layer, optional dense Layers, and an output layer.  \n",
    "Layer2 = True\n",
    "LSTM_units = 512 #LSTM layer units\n",
    "\n",
    "Layer3 = True\n",
    "layer3Act = \"relu\" #Hidden layer 3 activation\n",
    "hidden_nodes_layer3 = 512 #Hidden layer 3 node count\n",
    "\n",
    "Layer4 = True\n",
    "layer4Act = \"relu\" #Hidden layer 4 activation\n",
    "hidden_nodes_layer4 = 128 #Hidden layer 4 node count\n",
    "\n",
    "outputAct = \"sigmoid\" #Output layer activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0718316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data and labels\n",
    "y = df['label']\n",
    "X = df[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdc86a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Word2Vec model\n",
    "w2v_model = Word2Vec(sentences = X,\n",
    "    vector_size=init_vectorsize,         # Size of word vectors\n",
    "    window=window,                # Context window size\n",
    "    min_count=min_count,             # Minimum word frequency\n",
    "    workers=cores,           # Number of CPU cores to use\n",
    "    epochs=w2v_epochs,                # Number of training epochs\n",
    "    max_vocab_size=max_vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c4ea1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42773"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of keys/words in the model\n",
    "num_keys = len(w2v_model.wv.key_to_index)\n",
    "num_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "580decd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2v_model.wv.key_to_index.keys()) # This will print all keys that made it into the model. Warning... This can be very long...\n",
    "# print(X[0][:10]) # Check the first 10 word keys of article 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8ac4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer and transform X to numberical tokens. \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27c770b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD7CAYAAABpJS8eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAX5UlEQVR4nO3df0xV9/3H8dcVJss6u8aVixevDMilCBcQ7RVrshgru86lG6xqCMam12F3k9plP7q1IdkfHUk77rouq6tky03dctcmI3HJwJQWa+lcG1tHr9o2wtoShQSuN/wYGKtWFDzfP5rdr1YQ7i/kcp6Pf1o+9/x4vz34uud8zr1Hi2EYhgAAC96i210AAGBuEPgAYBIEPgCYBIEPACZB4AOASRD4AGASMwZ+XV2drFarSkpKbhh/4YUXVFhYKKfTqSeffDIy3tjYKIfDocLCQh06dCjxFQMAYpI+0wK7du3Sj370Iz388MORsX/+859qbW3Vhx9+qIyMDA0NDUmSuru71dzcrK6uLp09e1bf+ta39MknnygtLS15HQAAZmXGwN+wYYP6+vpuGPvjH/+o+vp6ZWRkSJKsVqskqbW1VbW1tcrIyFBeXp4cDoc6Ozu1fv36W+7j7rvvVm5ubmwdAIBJ9fX1aWRkZNbLzxj4U/nkk0/09ttv65e//KW+/OUv67nnntPatWsVCoV03333RZaz2+0KhUJTbsPv98vv90uS7rjjDgWDwVhKAQDTcrlcUS0fU+BPTExobGxMx44d03vvvaeamhqdOXNGUz2lwWKxTLkNr9crr9crKfqiAQDRi+lTOna7XVu3bpXFYlFFRYUWLVqkkZER2e129ff3R5YbGBhQdnZ2wooFAMQupsD//ve/rzfffFPS59M7V65c0d13362qqio1NzdrfHxcvb296unpUUVFRUILBgDEZsYpnR07dujIkSORM/iGhgbV1dWprq5OJSUlWrx4sQKBgCwWi5xOp2pqalRcXKz09HQ1NTXxCR0AmCcs8+HxyC6Xi5u2ABClaLOTb9oCgEkQ+ABgEgQ+AJgEgQ8AJrEgAj+3vu12lwAA896CCHwAwMwIfAAwCQIfAEyCwAcAkyDwAcAkCHwAMAkCHwBMgsAHAJMg8AHAJAh8ADAJAh8ATILABwCTIPABwCRmDPy6ujpZrVaVlJTc9Npzzz0ni8WikZGRyFhjY6McDocKCwt16NChxFYLAIjZjIG/a9cutbe33zTe39+vw4cPKycnJzLW3d2t5uZmdXV1qb29XXv27NHk5GRiKwYAxGTGwN+wYYOWLl160/jPfvYzPfvss7JYLJGx1tZW1dbWKiMjQ3l5eXI4HOrs7ExsxQCAmMQ0h3/w4EEtX75cq1atumE8FAppxYoVkZ/tdrtCodCU2/D7/XK5XHK5XBoeHo6lDABAFNKjXeHSpUt65pln9Prrr9/0mmEYN41dfwVwPa/XK6/XK0lyuVzRlgEAiFLUgX/69Gn19vZGzu4HBga0Zs0adXZ2ym63q7+/P7LswMCAsrOzE1ctACBmUU/plJaWamhoSH19ferr65PdbteJEye0bNkyVVVVqbm5WePj4+rt7VVPT48qKiqSUTcAIEozBv6OHTu0fv16ffzxx7Lb7dq/f/+0yzqdTtXU1Ki4uFhbtmxRU1OT0tLSElowACA2FmOqifc55nK5FAwGY14/t75Nfb4HElgRAMx/0WYn37QFAJMg8AHAJAh8ADAJAh8ATILABwCTIPABwCQIfAAwCQIfAEyCwAcAkyDwAcAkCHwAMAkCHwBMgsAHAJMg8AHAJAh8ADAJAh8ATILABwCTIPABwCRmDPy6ujpZrVaVlJRExp544gmtXLlSZWVlevDBB3Xu3LnIa42NjXI4HCosLNShQ4eSUjQAIHozBv6uXbvU3t5+w5jb7dapU6f04Ycf6p577lFjY6Mkqbu7W83Nzerq6lJ7e7v27NmjycnJ5FQOAIjKjIG/YcMGLV269IaxzZs3Kz09XZJ03333aWBgQJLU2tqq2tpaZWRkKC8vTw6HQ52dnUkoGwAQrbjn8P/85z/rO9/5jiQpFAppxYoVkdfsdrtCodCU6/n9frlcLrlcLg0PD8dbBgBgBnEF/jPPPKP09HTt3LlTkmQYxk3LWCyWKdf1er0KBoMKBoPKzMyMpwwAwCykx7piIBDQK6+8oo6Ojkio2+129ff3R5YZGBhQdnZ2/FUCAOIW0xl+e3u7fvOb3+jgwYP6yle+EhmvqqpSc3OzxsfH1dvbq56eHlVUVCSsWABA7GY8w9+xY4eOHDmikZER2e12NTQ0qLGxUePj43K73ZI+v3H7pz/9SU6nUzU1NSouLlZ6erqampqUlpaW9CYAADOzGFNNvM8xl8ulYDAY8/q59W3q8z2QwIoAYP6LNjv5pi0AmASBDwAmQeADgEkQ+ABgEgQ+AJgEgQ8AJkHgA4BJEPgAYBIEPgCYBIEPACZB4AOASRD4AGASBD4AmASBDwAmQeADgEkQ+ABgEgQ+AJgEgQ8AJjFj4NfV1clqtaqkpCQyNjo6KrfbrYKCArndbo2NjUVea2xslMPhUGFhoQ4dOpScqgEAUZsx8Hft2qX29vYbxnw+nyorK9XT06PKykr5fD5JUnd3t5qbm9XV1aX29nbt2bNHk5OTyakcABCVGQN/w4YNWrp06Q1jra2t8ng8kiSPx6OWlpbIeG1trTIyMpSXlyeHw6HOzs7EVw0AiFpMc/iDg4Oy2WySJJvNpqGhIUlSKBTSihUrIsvZ7XaFQqEpt+H3++VyueRyuTQ8PBxLGQCAKCT0pq1hGDeNWSyWKZf1er0KBoMKBoPKzMxMZBkAgCnEFPhZWVkKh8OSpHA4LKvVKunzM/r+/v7IcgMDA8rOzk5AmQCAeMUU+FVVVQoEApKkQCCg6urqyHhzc7PGx8fV29urnp4eVVRUJK5aAEDM0mdaYMeOHTpy5IhGRkZkt9vV0NCg+vp61dTUaP/+/crJydGBAwckSU6nUzU1NSouLlZ6erqampqUlpaW9CYAADOzGFNNvM8xl8ulYDAY8/q59W3q8z2QwIoAYP6LNjv5pi0AmASBDwAmQeADgEkQ+ABgEgQ+AJgEgQ8AJkHgA4BJEPgAYBIEPgCYBIEPACZB4AOASRD4AGASBD4AmASBDwAmQeADgEkQ+ABgEgQ+AJgEgQ8AJhFX4P/+97+X0+lUSUmJduzYocuXL2t0dFRut1sFBQVyu90aGxtLVK0AgDjEHPihUEh/+MMfFAwGderUKU1OTqq5uVk+n0+VlZXq6elRZWWlfD5fIusFAMQorjP8iYkJffbZZ5qYmNClS5eUnZ2t1tZWeTweSZLH41FLS0si6gQAxCnmwF++fLl+8YtfKCcnRzabTV/72te0efNmDQ4OymazSZJsNpuGhoYSViwAIHYxB/7Y2JhaW1vV29urs2fP6uLFi3r55Zdnvb7f75fL5ZLL5dLw8HCsZQAAZinmwH/jjTeUl5enzMxMfelLX9LWrVv1zjvvKCsrS+FwWJIUDodltVqnXN/r9SoYDCoYDCozMzPWMgAAsxRz4Ofk5OjYsWO6dOmSDMNQR0eHioqKVFVVpUAgIEkKBAKqrq5OWLHJllvfdrtLAICkSY91xXXr1mn79u1as2aN0tPTtXr1anm9Xl24cEE1NTXav3+/cnJydODAgUTWCwCIUcyBL0kNDQ1qaGi4YSwjI0MdHR1xFQUASDy+aQsAJkHgA4BJEPgAYBKmCXw+gQPA7EwT+ABgdgQ+AJgEgQ8AJkHgA4BJEPjTuP4mLzd8ASwEBD4AmASBDwAmYerAT+RUza22xZQQgPnA1IEfDUIbQKoj8AHAJAh8ADAJUwU+0zIAzMxUgQ8AZkbgA4BJmDLwZ5raYeoHwEIUV+CfO3dO27dv18qVK1VUVKR3331Xo6OjcrvdKigokNvt1tjYWKJqBQDEIa7A/8lPfqItW7boo48+0gcffKCioiL5fD5VVlaqp6dHlZWV8vl8iao1oaY7i+fsHsBCFXPgnz9/Xm+99ZZ2794tSVq8eLHuuusutba2yuPxSJI8Ho9aWloSUigAID4xB/6ZM2eUmZmpH/zgB1q9erUeeeQRXbx4UYODg7LZbJIkm82moaGhKdf3+/1yuVxyuVwaHh6OtQwAwCzFHPgTExM6ceKEHn30UZ08eVJ33HFHVNM3Xq9XwWBQwWBQmZmZsZaRMEzlAFjoYg58u90uu92udevWSZK2b9+uEydOKCsrS+FwWJIUDodltVoTU2mSJDvoeSMBMF/EHPjLli3TihUr9PHHH0uSOjo6VFxcrKqqKgUCAUlSIBBQdXV1YioFAMQlPZ6VX3jhBe3cuVNXrlxRfn6+/vKXv+jatWuqqanR/v37lZOTowMHDiSq1lvKrW9Tn++BpGw3kdtKRo0AMBtxBX55ebmCweBN4x0dHfFsFgCQBHEFfipgDh0APmfKRysAgBkR+FHgagFAKiPwb4GAB7CQEPgAYBIEPgCYxIIN/LmajrnVfpgSAjCfLNjAnw4hDMCsTBf48cqtb+NNA0BKIvABwCQI/NuIKwUAc2nBBf71IZrMQCWsAaSaBRf4AICpEfhK3Nl6NNvhCgHAXCPwk2CqMJ+rqSYAmA6BDwAmQeADgEkQ+HFI5GMVmOYBkGxxB/7k5KRWr16t7373u5Kk0dFRud1uFRQUyO12a2xsLO4iZ4vQBIDpxR34e/fuVVFRUeRnn8+nyspK9fT0qLKyUj6fL95dzHvckAWQCuIK/IGBAbW1temRRx6JjLW2tsrj8UiSPB6PWlpa4ioQAJAYcQX+T3/6Uz377LNatOj/NzM4OCibzSZJstlsGhoamnJdv98vl8sll8ul4eHheMoAAMxCzIH/yiuvyGq16t57741pfa/Xq2AwqGAwqMzMzFjLmFKqTqukat0AUkPMgX/06FEdPHhQubm5qq2t1ZtvvqmHHnpIWVlZCofDkqRwOCyr1ZqwYheCL4Y6IQ9grsQc+I2NjRoYGFBfX5+am5u1adMmvfzyy6qqqlIgEJAkBQIBVVdXJ6xYAEDsEv45/Pr6eh0+fFgFBQU6fPiw6uvrE72LBWe6s3zO/gEkUnoiNrJx40Zt3LhRkvT1r39dHR0didgsrpNb36Y+3wO3uwwAKYxv2qYAzvQBJAKBDwAmQeDPU5zVA0g0Ah8ATILABwCTIPABwCQIfAAwiYR8Dh+Jw81aAMnCGT4AmASBP88l8p9RBGBuBD4AmASBDwAmQeADgEkQ+CmGeXsAsSLwUxT/chaAaBH4CwBhD2A2CHwAMAkCP4XM9kyeM34AU4k58Pv7+3X//ferqKhITqdTe/fulSSNjo7K7XaroKBAbrdbY2NjCSsWseNNAEDMgZ+enq7f/e53+s9//qNjx46pqalJ3d3d8vl8qqysVE9PjyorK+Xz+RJZL8S3bwHEJubAt9lsWrNmjSRpyZIlKioqUigUUmtrqzwejyTJ4/GopaUlIYUCAOKTkKdl9vX16eTJk1q3bp0GBwdls9kkff6mMDQ0NOU6fr9ffr9fkjQ8PJyIMkwp2jP6/y3f53sgGeUAmMfivml74cIFbdu2Tc8//7zuvPPOWa/n9XoVDAYVDAaVmZkZbxnQjeHP1A6AL4or8K9evapt27Zp586d2rp1qyQpKytL4XBYkhQOh2W1WuOvEnEh/AFIcQS+YRjavXu3ioqK9Pjjj0fGq6qqFAgEJEmBQEDV1dXxVwkAiFvMc/hHjx7VSy+9pNLSUpWXl0uSfv3rX6u+vl41NTXav3+/cnJydODAgUTVCgCIQ8yB/81vflOGYUz5WkdHR8wFYW5xExcwD75pu4Axdw/gegQ+AJgEgY+I3Po2rgqABYzAX+AIcAD/Q+ADgEkQ+JA09ZXATFM8XD0AqYXANynCGjAfAh8ATILABwCTIPBNbLbz80z/AAsDgY9Z+V/o81l9IHUR+CaSjKAm/IHUQeADgEkQ+LhJIs/aZ9oWVwjA3CHwEbfr5/Wnmuv/4g3g2YZ8LPcLFuIbyELsCbcHgY9Zm+mTO9EEUzzf4OUTREBsCHwAMAkCH3NiqumZqc7UE/FMn2injZIpWdufzXa5+km8VP8zTVrgt7e3q7CwUA6HQz6fL1m7wQI1U/BHG/IzvaFM91qs9UWzzq3+O9vtRPN6NMsn47WFbL73nZTAn5yc1GOPPabXXntN3d3d+tvf/qbu7u5k7ArzUKyBk+wbtF+8oXyrewG3Cu/pbkZ/cd1b3cC+1ZvXTPXPZpnprp4S8QYTz/2bmf5Mp6t3qp9nu/9o15lpn7P9vZjNvaa5foNISuB3dnbK4XAoPz9fixcvVm1trVpbW5OxKwDALFkMwzASvdG///3vam9v14svvihJeumll/Tvf/9b+/btiyzj9/vl9/slSR999JFWrlwZ076Gh4eVmZkZf9HzyELriX7mN/qZ327VT19fn0ZGRma9rfREFXW9qd5DLBbLDT97vV55vd649+VyuRQMBuPeznyy0Hqin/mNfua3RPaTlCkdu92u/v7+yM8DAwPKzs5Oxq4AALOUlMBfu3atenp61NvbqytXrqi5uVlVVVXJ2BUAYJaSMqWTnp6uffv26dvf/rYmJydVV1cnp9OZjF0lZFpovlloPdHP/EY/81si+0nKTVsAwPzDN20BwCQIfAAwiZQO/FR9fENubq5KS0tVXl4ul8slSRodHZXb7VZBQYHcbrfGxsYiyzc2NsrhcKiwsFCHDh26XWVH1NXVyWq1qqSkJDIWS/3Hjx9XaWmpHA6HfvzjH0/5cd65MFU/v/rVr7R8+XKVl5ervLxcr776auS1+d5Pf3+/7r//fhUVFcnpdGrv3r2SUvcYTddPqh6jy5cvq6KiQqtWrZLT6dRTTz0laY6Oj5GiJiYmjPz8fOP06dPG+Pi4UVZWZnR1dd3usmblG9/4hjE8PHzD2BNPPGE0NjYahmEYjY2NxpNPPmkYhmF0dXUZZWVlxuXLl40zZ84Y+fn5xsTExJzXfL1//etfxvHjxw2n0xkZi6X+tWvXGu+8845x7do1Y8uWLcarr746980YU/fz1FNPGb/97W9vWjYV+jl79qxx/PhxwzAM4/z580ZBQYHR1dWVssdoun5S9Rhdu3bN+PTTTw3DMIwrV64YFRUVxrvvvjsnxydlz/AX2uMbWltb5fF4JEkej0ctLS2R8draWmVkZCgvL08Oh0OdnZ23sVJpw4YNWrp06Q1j0dYfDod1/vx5rV+/XhaLRQ8//HBknbk2VT/TSYV+bDab1qxZI0lasmSJioqKFAqFUvYYTdfPdOZ7PxaLRV/96lclSVevXtXVq1dlsVjm5PikbOCHQiGtWLEi8rPdbr/lL8F8YrFYtHnzZt17772Rx0sMDg7KZrNJ+vwXfGhoSFLq9Blt/aFQSHa7/abx+WTfvn0qKytTXV1d5PI61frp6+vTyZMntW7dugVxjK7vR0rdYzQ5Oany8nJZrVa53e45Oz4pG/jGLB7fMF8dPXpUJ06c0Guvvaampia99dZb0y6byn1K09c/3/t69NFHdfr0ab3//vuy2Wz6+c9/Lim1+rlw4YK2bdum559/Xnfeeee0y6VKT1/sJ5WPUVpamt5//30NDAyos7NTp06dmnbZRPaTsoGfyo9v+F+dVqtVDz74oDo7O5WVlaVwOCxJCofDslqtklKnz2jrt9vtGhgYuGl8vsjKylJaWpoWLVqkH/7wh5FptFTp5+rVq9q2bZt27typrVu3SkrtYzRdP6l8jCTprrvu0saNG9Xe3j4nxydlAz9VH99w8eJFffrpp5H/f/3111VSUqKqqioFAgFJUiAQUHV1tSSpqqpKzc3NGh8fV29vr3p6elRRUXHb6p9OtPXbbDYtWbJEx44dk2EY+utf/xpZZz743188SfrHP/4R+QRPKvRjGIZ2796toqIiPf7445HxVD1G0/WTqsdoeHhY586dkyR99tlneuONN7Ry5cq5OT6JuvN8O7S1tRkFBQVGfn6+8fTTT9/ucmbl9OnTRllZmVFWVmYUFxdH6h4ZGTE2bdpkOBwOY9OmTcZ///vfyDpPP/20kZ+fb9xzzz237ZMf16utrTWWLVtmpKenG8uXLzdefPHFmOp/7733DKfTaeTn5xuPPfaYce3atdvRzpT9PPTQQ0ZJSYlRWlpqfO973zPOnj0bWX6+9/P2228bkozS0lJj1apVxqpVq4y2traUPUbT9ZOqx+iDDz4wysvLjdLSUsPpdBoNDQ2GYcSWAdH2w6MVAMAkUnZKBwAQHQIfAEyCwAcAkyDwAcAkCHwAMAkCHwBMgsAHAJP4P0zkZNRveAMlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the length of all article's vectors and saving the distribution in a list.\n",
    "vector_lengths = [len(x) for x in X]\n",
    "plt.hist(vector_lengths, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d73cd2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the longest vector is 2924 words long\n",
      "Max length is 1000, reducing and padding to 1000 terms\n"
     ]
    }
   ],
   "source": [
    "# Setting the W2V vectors to be all the same length, and if they are over the set max, truncates them. \n",
    "longest_vector = np.max(vector_lengths)\n",
    "print(f'the longest vector is {longest_vector} words long')\n",
    "\n",
    "if longest_vector >= max_vectorsize:\n",
    "    longest_vector = max_vectorsize\n",
    "    print(f'Max length is {longest_vector}, reducing and padding to {max_vectorsize} terms')\n",
    "\n",
    "#Making all news of size maxlen defined above\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=longest_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bf2a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the training and testing split as per ratio above.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbf55cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the W2V model weight matrix to input into the NN initialization layer.\n",
    "weight_matrix = w2v_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1591289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42773, 150)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d83f832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1000, 150)         6415950   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               142848    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              264192    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,872,591\n",
      "Trainable params: 1,456,641\n",
      "Non-trainable params: 6,415,950\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "# First input hidden layer using model weight matrix, set to untrainable.\n",
    "nn.add(tf.keras.layers.Embedding(num_keys, output_dim=init_vectorsize, weights=[weight_matrix], input_length=longest_vector, trainable=False))\n",
    "# Second layer, using an LSTM layer that relies on the first weighted layer.\n",
    "if Layer2 == True:\n",
    "    nn.add(tf.keras.layers.LSTM(units=LSTM_units))\n",
    "# Third hidden layer, a standard layer\n",
    "if Layer3 == True:\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=layer3Act))\n",
    "# Fourth hidden layer, a standard layer\n",
    "if Layer4 == True:\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=layer4Act))\n",
    "# Output layer, binary\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=outputAct))\n",
    "\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "#del embedding_vectors\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2be2fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to train model on GPU...\n",
      "Epoch 1/25\n",
      "GPU error. Defaulting to use CPU.\n",
      "Epoch 1/25\n",
      "158/158 [==============================] - 98s 616ms/step - loss: 0.5080 - accuracy: 0.7370\n",
      "Epoch 2/25\n",
      "158/158 [==============================] - 94s 597ms/step - loss: 0.3476 - accuracy: 0.8449\n",
      "Epoch 3/25\n",
      "158/158 [==============================] - 95s 600ms/step - loss: 0.2149 - accuracy: 0.9132\n",
      "Epoch 4/25\n",
      "158/158 [==============================] - 96s 610ms/step - loss: 0.1096 - accuracy: 0.9591\n",
      "Epoch 5/25\n",
      "158/158 [==============================] - 93s 589ms/step - loss: 0.0631 - accuracy: 0.9776\n",
      "Epoch 6/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0362 - accuracy: 0.9881\n",
      "Epoch 7/25\n",
      "158/158 [==============================] - 91s 574ms/step - loss: 0.0658 - accuracy: 0.9768\n",
      "Epoch 8/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0195 - accuracy: 0.9942\n",
      "Epoch 9/25\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.0202 - accuracy: 0.9940\n",
      "Epoch 10/25\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.0416 - accuracy: 0.9865\n",
      "Epoch 11/25\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.0153 - accuracy: 0.9954\n",
      "Epoch 12/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0260 - accuracy: 0.9923\n",
      "Epoch 13/25\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.0097 - accuracy: 0.9960\n",
      "Epoch 14/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0207 - accuracy: 0.9934\n",
      "Epoch 15/25\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.0078 - accuracy: 0.9976\n",
      "Epoch 16/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 8.2997e-04 - accuracy: 0.9998\n",
      "Epoch 17/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0076 - accuracy: 0.9984\n",
      "Epoch 18/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0122 - accuracy: 0.9960\n",
      "Epoch 19/25\n",
      "158/158 [==============================] - 91s 575ms/step - loss: 0.0066 - accuracy: 0.9974\n",
      "Epoch 20/25\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.0105 - accuracy: 0.9970\n",
      "Epoch 21/25\n",
      "158/158 [==============================] - 91s 577ms/step - loss: 0.0236 - accuracy: 0.9942\n",
      "Epoch 22/25\n",
      "158/158 [==============================] - 91s 578ms/step - loss: 0.0180 - accuracy: 0.9954\n",
      "Epoch 23/25\n",
      "158/158 [==============================] - 91s 578ms/step - loss: 0.0159 - accuracy: 0.9952\n",
      "Epoch 24/25\n",
      "158/158 [==============================] - 91s 577ms/step - loss: 0.0133 - accuracy: 0.9958\n",
      "Epoch 25/25\n",
      "158/158 [==============================] - 91s 578ms/step - loss: 0.0189 - accuracy: 0.9962\n"
     ]
    }
   ],
   "source": [
    "# Train the model, try/excepts used to select proper hardward device. \n",
    "if len(tf.config.list_physical_devices('GPU')) >= 1:\n",
    "    try:\n",
    "        print('Attempting to train model on GPU...')\n",
    "        with tf.device('/GPU:0'):  #Note: I set up tensorflow with GPU. Change this to CPU:0 in order to obtain standard use. \n",
    "            fit_model = nn.fit(X_train,y_train,epochs=train_epochs)\n",
    "    except:\n",
    "        print('GPU error. Defaulting to use CPU.')\n",
    "        with tf.device('/CPU:0'):  \n",
    "            fit_model = nn.fit(X_train,y_train,epochs=train_epochs)\n",
    "else:\n",
    "    print('No GPU detected. Defaulting to use CPU.')\n",
    "    with tf.device('/CPU:0'): \n",
    "        fit_model = nn.fit(X_train,y_train,epochs=train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d04b70c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to evaluate model on GPU...\n",
      "GPU error. Defaulting to use CPU.\n",
      "40/40 - 8s - loss: 1.0272 - accuracy: 0.8118 - 8s/epoch - 194ms/step\n",
      "40/40 [==============================] - 8s 195ms/step\n",
      "Loss: 1.0272148847579956, Accuracy: 0.811755359172821\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data, try/excepts used to select proper hardward device.\n",
    "if len(tf.config.list_physical_devices('GPU')) >= 1:\n",
    "    try:\n",
    "        print('Attempting to evaluate model on GPU...')\n",
    "        with tf.device('/GPU:0'):  #Note: I set up tensorflow with GPU. Change this to CPU:0 in order to obtain standard use. \n",
    "            model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "            predictions = nn.predict(X_test)\n",
    "\n",
    "    except:\n",
    "        print('GPU error. Defaulting to use CPU.')\n",
    "        with tf.device('/CPU:0'):  \n",
    "            model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "            predictions = nn.predict(X_test)\n",
    "\n",
    "else:\n",
    "    print('No GPU detected. Defaulting to use CPU.')\n",
    "    with tf.device('/CPU:0'): \n",
    "        model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "        predictions = nn.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31fc5e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>115</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>512</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          115          510\n",
       "Actual 1          512          122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collecting the models confusion matrix\n",
    "preds =  [np.round(x) % 2 == 0 for x in predictions]\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07c2483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file using the data source, accuracy, and epochs as stratifiers. \n",
    "nn.save(f'models/Model_W2V_Project4_{source}_{model_accuracy:.2f}acc_{train_epochs}epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba00afd",
   "metadata": {},
   "source": [
    "#### Extra links:\n",
    "https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.2015.145052010082\n",
    "\n",
    "https://sandipanweb.wordpress.com/2017/06/03/some-nlp-probabilistic-context-free-grammar-pcfg-and-cky-parsing/\n",
    "\n",
    "https://gawron.sdsu.edu/compling/course_core/lectures/prob_parse.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e07cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
